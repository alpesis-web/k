<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tech | Kelly Chan]]></title>
  <link href="http://k.arttechresearch.com/categories/tech/atom.xml" rel="self"/>
  <link href="http://k.arttechresearch.com/"/>
  <updated>2015-10-12T12:44:35+08:00</updated>
  <id>http://k.arttechresearch.com/</id>
  <author>
    <name><![CDATA[Kelly Chan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Installing Openstack Devstack]]></title>
    <link href="http://k.arttechresearch.com/tech/2015/10/03/installing-openstack-devstack/"/>
    <updated>2015-10-03T12:33:38+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2015/10/03/installing-openstack-devstack</id>
    <content type="html"><![CDATA[<h3>Prequisition</h3>

<p>Softwares:</p>

<p>&ndash; (VM) <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox 5.0</a><br/>
&ndash; (OS) <a href="http://www.ubuntu.com/download/server">Ubuntu Server 14.04.3</a><br/>
&ndash; (SW) <a href="https://github.com/openstack-dev/devstack">Openstack</a></p>

<p>Steps:</p>

<p>&ndash; Installing VirtualBox<br/>
&ndash; Installing Ubuntu 14.04.03<br/>
&ndash; Installing Openstack<br/>
&ndash; Testing</p>

<h3>1. Installing VirtualBox</h3>

<p>Go to <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a>, download and install on the local machine.</p>

<h3>2. Installing Ubuntu</h3>

<p>Go to <a href="http://www.ubuntu.com/download/server">Ubuntu</a> page, download the ubuntu server 14.04.03 (NOTE: Preferring 14.04.03) iso image.</p>

<p>Once done, open VirtualBox, create a vm for Ubuntu server. The vm settings are:<br/>
&ndash; memory: 2GB<br/>
&ndash; storage: 100GB<br/>
&ndash; <strong>network: briaged adapter (IMPORTANT), promiscuous mode with ALLOW ALL</strong></p>

<p>Then, attach Ubuntu server iso file to the drive, double click the vm, the system will be jumped to the installation guide.</p>

<p>When installing Ubuntu, the installation mode should be:<br/>
&ndash; <strong>multi-server with MAAS (IMPORTANT)</strong></p>

<p>Once done, restart and config the system:</p>

<pre><code class="bash config system">    $ sudo apt-get update &amp;&amp; upgrade
    $ sudo apt-get install git
    $ git config --global user.name "username"
    $ git config --global user.email "user@example.com"
</code></pre>

<h3>3. Installing Openstack</h3>

<p>Go to <a href="https://github.com/openstack-dev/devstack">Openstack</a>, download the repo and install:</p>

<pre><code class="bash installing openstack">    # create a project folder and download the repo
    $ cd /
    $ sudo mkdir openstack
    $ cd openstack
    $ git clone https://github.com/openstack-dev/devstack.git
    $ cd devstack
    $ git checkout stable/juno

    # config user
    $ sudo ./tools/create-stack-user.sh
    $ sudo chown -R stack:stack /openstack

    # install
    $ ./stack.sh
</code></pre>

<p>Trouble Shooting</p>

<p><strong>(1)</strong> If something wrong, try <code>./unstack.sh</code> and then <code>./clean.sh</code>, restart and then <code>./stack.sh</code> again.</p>

<p><strong>(2)</strong> If re-run <code>./stack.sh</code> and the python packages flag some issue, try to delete <code>/opt/stack/xxx</code>, and re-run <code>./stack.sh</code>.</p>

<h3>4. Testing</h3>

<p>Open a browser, enter the dashboard with <code>http://ip_address</code>, login with <code>admin/password</code> (NOTE: password was set at previous).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bike Sharing Demand]]></title>
    <link href="http://k.arttechresearch.com/tech/2014/06/03/bike-sharing-demand/"/>
    <updated>2014-06-03T14:49:22+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2014/06/03/bike-sharing-demand</id>
    <content type="html"><![CDATA[<p>The project is to predict the count of bikes sharing per day per hour.</p>

<ul>
<li><a href="http://www.kaggle.com/c/bike-sharing-demand">description</a>: bike sharing demand at Kaggle</li>
<li><a href="http://www.kaggle.com/c/bike-sharing-demand/data">data</a>: data source of this project at Kaggle</li>
<li><a href="https://github.com/KellyChan/kaggle-bike-sharing-demand">codes</a>: the source codes of this project are available at Github</li>
</ul>


<h2>1. Features</h2>

<p>prediction: count = casual + registered</p>

<pre><code>count - number of total rentals
casual - number of non-registered user rentals initiated
registered - number of registered user rentals initiated
</code></pre>

<p>features</p>

<pre><code>datetime - hourly date + timestamp  
season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
holiday - whether the day is considered a holiday
workingday - whether the day is neither a weekend nor holiday
weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy 
          2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist 
          3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 
          4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
temp - temperature in Celsius
atemp - "feels like" temperature in Celsius
humidity - relative humidity
windspeed - wind speed
</code></pre>

<h2>2. Evaluation</h2>

<p>Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as</p>

<p>$$\sqrt{\frac 1n \sum_1<sup>n</sup> (log(p_i + 1) - log(a_i + 1))<sup>2</sup> }$$</p>

<p>Where:</p>

<ul>
<li>$$n$$ is the number of hours in the test set</li>
<li>$$p_i$$ is your predicted count</li>
<li>$$a_i$$ is the actual count</li>
<li>$$log(x)$$ is the natural logarithm</li>
</ul>


<h2>3. Algorithms</h2>

<p>algorithms:</p>

<ul>
<li>decision tree regressor</li>
<li>extra tree regressor</li>
<li>random forest regressor</li>
</ul>


<h2>4. Practice</h2>

<pre><code class="python">dataPath = "G:/vimFiles/python/kaggle/201406-bike/data/"
outPath = "G:/vimFiles/python/kaggle/201406-bike/src/outputs/results/"

import pandas as pd

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import RandomForestRegressor

def loadData(datafile):
    return pd.read_csv(datafile)

def splitDatetime(data):
    sub = pd.DataFrame(data.datetime.str.split(' ').tolist(), columns = "date time".split())
    date = pd.DataFrame(sub.date.str.split('-').tolist(), columns="year month day".split())
    time = pd.DataFrame(sub.time.str.split(':').tolist(), columns = "hour minute second".split())
    data['year'] = date['year']
    data['month'] = date['month']
    data['day'] = date['day']
    data['hour'] = time['hour'].astype(int)
    return data

def createDecisionTree():
    est = DecisionTreeRegressor()
    return est

def createRandomForest():
    est = RandomForestRegressor(n_estimators=100)
    return est

def createExtraTree():
    est = ExtraTreesRegressor()
    return est

def predict(est, train, test, features, target):

    est.fit(train[features], train[target])

    with open(outPath + "submission-randomforest.csv", 'wb') as f:
        f.write("datetime,count\n")

        for index, value in enumerate(list(est.predict(test[features]))):
            f.write("%s,%s\n" % (test['datetime'].loc[index], int(value)))


def main():

    train = loadData(dataPath + "train.csv")
    test = loadData(dataPath + "test.csv")

    train = splitDatetime(train)
    test = splitDatetime(test)

    target = 'count'
    features = [col for col in train.columns if col not in ['datetime', 'casual', 'registered', 'count']]

    est = createRandomForest()
    predict(est, train, test, features, target)



if __name__ == "__main__":
    main()
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Forest Cover Type Prediction]]></title>
    <link href="http://k.arttechresearch.com/tech/2014/05/28/forest-cover-type-prediction/"/>
    <updated>2014-05-28T15:00:00+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2014/05/28/forest-cover-type-prediction</id>
    <content type="html"><![CDATA[<p>This project is to predict the forest cover type with the cartography data provided by US Geological Survey and USFS.</p>

<ul>
<li><a href="http://www.kaggle.com/c/forest-cover-type-prediction">description</a>: forest cover type prediction at Kaggle</li>
<li><a href="http://www.kaggle.com/c/forest-cover-type-prediction/data">data</a>: data source of this project at Kaggle</li>
<li><a href="https://github.com/KellyChan/kaggle-forest-cover-type-prediction">codes</a>: the source codes of this project are available at Github</li>
</ul>


<h2>1. Working Process</h2>

<ul>
<li>step 1. combining binary variables (Wilderness_Area and Soil_Type) as one</li>
<li>step 2. applying machine learning algorithms (random forest)</li>
<li>step 3. evaluating the accuracy of prediction</li>
</ul>


<h2>2. Preprocessing</h2>

<p>combining binary variables (Wilderness_Area, Soil_Type)</p>

<h2>3. Classification</h2>

<p>datasets</p>

<ul>
<li>raw data</li>
<li>punched data</li>
</ul>


<p>algorithms</p>

<ul>
<li>decision tree</li>
<li>random forest</li>
<li>extra tree: raw data worked much better than punched data.</li>
<li>adaboost</li>
</ul>


<h2>4. Evaluation</h2>

<p>% of corrections</p>

<h2>5. Practice</h2>

<h3>step1. data punching</h3>

<p>combining binary variables - Wilderness_Area, Soil_Type</p>

<p>NOTE: Pandas is good at handling with data by batch, it is much faster than processing each record one by one.</p>

<pre><code class="python">dataPath = "path/kaggle/201405-Forest/data/"
outPath = "path/kaggle/201405-Forest/src/outputs/data/"

import pandas as pd

def loadData(datafile):
    return pd.read_csv(datafile)

def createWildernessArea(data):

    for i in range(4):
        coded = {'1': i+1, '0': 0}
        col = 'Wilderness_Area' + str(i+1)
        data[col] = data[col].astype(str).map(coded)

    data['Wilderness_Area'] = data['Wilderness_Area1'] + \
                              data['Wilderness_Area2'] + \
                              data['Wilderness_Area3'] + \
                              data['Wilderness_Area4']

    for i in range(4):
        col = 'Wilderness_Area' + str(i+1)
        data = data.drop(col, 1)

    return data

def createSoilType(data):

    for i in range(40):
        coded = {'1': i+1, '0': 0}
        col = 'Soil_Type' + str(i+1)
        data[col] = data[col].astype(str).map(coded)    

    data['Soil_Type'] = 0
    for i in range(40):
        col = 'Soil_Type' + str(i+1)
        data['Soil_Type'] += data[col] 

    for i in range(40):
        col = 'Soil_Type' + str(i+1)
        data = data.drop(col, 1)

    return data

def main():
    train = loadData(dataPath + "train.csv")
    test = loadData(dataPath + "test.csv")

    train = createWildernessArea(train)
    train = createSoilType(train)
    train.to_csv(outPath + "train.csv")

    test = createWildernessArea(test)
    test = createSoilType(test)
    test.to_csv(outPath + "test.csv")


if __name__ == '__main__':
    main()
</code></pre>

<h3>step2. classification with trees</h3>

<pre><code class="python">"""
Function Tree

- chooseDataset
     |------ loadData

- chooseAlgorithms
     |------ createDecisionTree
     |------ createRandomForest
     |------ createExtraTree
     |------ createAdaBoost

"""

rawPath = "G:/vimFiles/python/kaggle/201405-Forest/data/"
dataPath = "G:/vimFiles/python/kaggle/201405-Forest/src/outputs/data/"
outPath = "G:/vimFiles/python/kaggle/201405-Forest/src/outputs/results/"

import pandas as pd

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier

def loadData(datafile):
    return pd.read_csv(datafile)


def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createRandomForest():
    clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createExtraTree():
    clf = ExtraTreesClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createAdaBoost():
    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf = AdaBoostClassifier(dt, n_estimators=300)
    return clf

def classify(clf, train, cols, target, test, filePath):
    clf.fit(train[cols], train[target])

    with open(filePath, "wb") as outfile:
        outfile.write("Id,Cover_Type\n")
        for index, value in enumerate(list(clf.predict(test[cols]))):
            outfile.write("%s,%s\n" % (test['Id'].loc[index], value))


def chooseDataset(dataset):
    if dataset == 1:
        train = loadData(rawPath + "train.csv")
        test = loadData(rawPath + "test.csv")
    elif dataset == 2:
        train = loadData(dataPath + "train.csv")
        test = loadData(dataPath + "test.csv")
    return train, test

def chooseAlgorithms(algo, train, test, target, features):

    if algo == 0:

        clf = createDecisionTree()
        classify(clf, train, features, target, test, outPath+"submission-decisiontree.csv")        

        clf = createRandomForest()
        classify(clf, train, features, target, test, outPath+"submission-randomforest.csv")

        clf = createExtraTree()
        classify(clf, train, features, target, test, outPath+"submission-extratree.csv")

        clf = createAdaBoost()
        classify(clf, train, features, target, test, outPath+"submission-adaboost.csv")

    elif algo == 1:
        clf = createDecisionTree()
        classify(clf, train, features, target, test, outPath+"submission-decisiontree.csv")

    elif algo == 2:
        clf = createRandomForest()
        classify(clf, train, features, target, test, outPath+"submission-randomforest.csv")

    elif algo == 3:
        clf = createExtraTree()
        classify(clf, train, features, target, test, outPath+"submission-extratree.csv")

    elif algo == 4:
        clf = createAdaBoost()
        classify(clf, train, features, target, test, outPath+"submission-adaboost.csv")


def main():

    """ selections: dataset and algorithms

    dataset:
    - 1: raw data - Wilderness_Area, Soil_Type are binary codes
    - 2: punched data - Wilderness_Area, Soil_Type are combined

    algo: algorithms
    - 0: all
    - 1: decision tree
    - 2: random forest
    - 3: extra tree
    - 4: adaboost
    """
    dataset = 1
    algo = 1

    train, test = chooseDataset(dataset)

    target = 'Cover_Type'
    features = [col for col in train.columns if col not in ['Id', 'Cover_Type', 'Unnamed']]

    chooseAlgorithms(algo, train, test, target, features)



if __name__ == '__main__':
    main()
</code></pre>

<h2>6. Data formats</h2>

<p>predict(submission): <code>Id</code>, <code>Cover_Type</code></p>

<pre><code>Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation

1 - Spruce/Fir
2 - Lodgepole Pine
3 - Ponderosa Pine
4 - Cottonwood/Willow
5 - Aspen
6 - Douglas-fir
7 - Krummholz
</code></pre>

<p>train/test: <code>Id</code></p>

<p>features</p>

<pre><code>Elevation - Elevation in meters
Aspect - Aspect in degrees azimuth
Slope - Slope in degrees

Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features
Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features
Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway
Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points

Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice
Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice
Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice

Wilderness_Area (4 areas, 0 = absence, 1 = presence) - Wilderness area designation
- 1 - Rawah Wilderness Area
- 2 - Neota Wilderness Area
- 3 - Comanche Peak Wilderness Area
- 4 - Cache la Poudre Wilderness Area

Soil_Type (40 types, 0 = absence, 1 = presence) - Soil Type designation
- 1 Cathedral family - Rock outcrop complex, extremely stony. 
- 2 Vanet - Ratake families complex, very stony.
- 3 Haploborolis - Rock outcrop complex, rubbly.
- 4 Ratake family - Rock outcrop complex, rubbly.
- 5 Vanet family - Rock outcrop complex complex, rubbly.
- 6 Vanet - Wetmore families - Rock outcrop complex, stony.
- 7 Gothic family.
- 8 Supervisor - Limber families complex.
- 9 Troutville family, very stony.
- 10 Bullwark - Catamount families - Rock outcrop complex, rubbly.
- 11 Bullwark - Catamount families - Rock land complex, rubbly.
- 12 Legault family - Rock land complex, stony.
- 13 Catamount family - Rock land - Bullwark family complex, rubbly.
- 14 Pachic Argiborolis - Aquolis complex.
- 15 unspecified in the USFS Soil and ELU Survey.
- 16 Cryaquolis - Cryoborolis complex.
- 17 Gateview family - Cryaquolis complex.
- 18 Rogert family, very stony.
- 19 Typic Cryaquolis - Borohemists complex.
- 20 Typic Cryaquepts - Typic Cryaquolls complex.
- 21 Typic Cryaquolls - Leighcan family, till substratum complex.
- 22 Leighcan family, till substratum, extremely bouldery.
- 23 Leighcan family, till substratum - Typic Cryaquolls complex.
- 24 Leighcan family, extremely stony.
- 25 Leighcan family, warm, extremely stony.
- 26 Granile - Catamount families complex, very stony.
- 27 Leighcan family, warm - Rock outcrop complex, extremely stony.
- 28 Leighcan family - Rock outcrop complex, extremely stony.
- 29 Como - Legault families complex, extremely stony.
- 30 Como family - Rock land - Legault family complex, extremely stony.
- 31 Leighcan - Catamount families complex, extremely stony.
- 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.
- 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.
- 34 Cryorthents - Rock land complex, extremely stony.
- 35 Cryumbrepts - Rock outcrop - Cryaquepts complex.
- 36 Bross family - Rock land - Cryumbrepts complex, extremely stony.
- 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.
- 38 Leighcan - Moran families - Cryaquolls complex, extremely stony.
- 39 Moran family - Cryorthents - Leighcan family complex, extremely stony.
- 40 Moran family - Cryorthents - Rock land complex, extremely stony.
</code></pre>

<h2>Reference</h2>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Aspect_(geography">Aspect Geography</a>)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Text Classification With Naive Bayes]]></title>
    <link href="http://k.arttechresearch.com/tech/2014/05/24/text-classification-with-naive-bayes/"/>
    <updated>2014-05-24T01:05:22+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2014/05/24/text-classification-with-naive-bayes</id>
    <content type="html"><![CDATA[<h2>1. Working Process</h2>

<p>general process</p>

<pre><code>raw text --&gt; keywords --&gt; features --&gt; train --&gt; clasifier
raw text              --&gt; features --&gt; test  --&gt; classify
</code></pre>

<p>example: sentimental analysis for tweets</p>

<pre><code>positive/negative tweets --&gt; keywords --&gt; features --&gt; train --&gt; clasifier
unlabeled tweets                      --&gt; features --&gt; test  --&gt; classify
</code></pre>

<h2>2. Feature Extraction</h2>

<p>keyword extraction: wordFreqDict</p>

<pre><code>wordFreqDict = {keyword1: 90,
                keyword2: 30,
                keyword3: 3,
                ...}
</code></pre>

<p>feature extraction: True/False table</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> tweet                </th>
<th style="text-align:left;"> keyword1 </th>
<th style="text-align:left;"> keyword2 </th>
<th style="text-align:left;"> keyword3 </th>
<th style="text-align:left;"> &hellip; </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> &ldquo;I like this dog.&rdquo;   </td>
<td style="text-align:left;"> True     </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> True     </td>
<td style="text-align:left;"> &hellip; </td>
</tr>
<tr>
<td style="text-align:left;"> &ldquo;Beautiful morning.&rdquo; </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> &hellip; </td>
</tr>
<tr>
<td style="text-align:left;"> &hellip;                  </td>
<td style="text-align:left;"> &hellip;      </td>
<td style="text-align:left;"> &hellip;      </td>
<td style="text-align:left;"> &hellip;      </td>
<td style="text-align:left;"> &hellip; </td>
</tr>
</tbody>
</table>


<p>.</p>

<p>train data: features + label</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> tweet                </th>
<th style="text-align:left;"> keyword1 </th>
<th style="text-align:left;"> keyword2 </th>
<th style="text-align:left;"> keyword3 </th>
<th style="text-align:left;"> &hellip; </th>
<th style="text-align:left;"> label    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> &ldquo;I like this dog.&rdquo;   </td>
<td style="text-align:left;"> True     </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> True     </td>
<td style="text-align:left;"> &hellip; </td>
<td style="text-align:left;"> Positive </td>
</tr>
<tr>
<td style="text-align:left;"> &ldquo;Beautiful morning.&rdquo; </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> False    </td>
<td style="text-align:left;"> &hellip; </td>
<td style="text-align:left;"> Positive </td>
</tr>
<tr>
<td style="text-align:left;"> &hellip;                  </td>
<td style="text-align:left;"> &hellip;      </td>
<td style="text-align:left;"> &hellip;      </td>
<td style="text-align:left;"> &hellip;      </td>
<td style="text-align:left;"> &hellip; </td>
<td style="text-align:left;"> &hellip;      </td>
</tr>
</tbody>
</table>


<p>.</p>

<h2>3. Practice</h2>

<p>function construct</p>

<pre><code>    |----- [test-features]: extractFeatures(dataX, keywords)
    |----- [train]: createTrain(fullData, keywords)
              |----- [features]: extractFeatures(dataX, keywords)
              |----- [keywords]: createKeywords(fullData)
                        |----- [fullData]: generateFullData(data1, data2)
                                     |----- [rawData]: loadData()
</code></pre>

<p>data format</p>

<pre><code>- rawData: [('word word ...', 'label'), ... ]
- fullData: [([words], label), ([words], label), ...]

- features: {word: True/False}
- train: [({features}, label), ({features}, label), ...]
- test: ['word word ...', 'word word ...', ...]
</code></pre>

<p>source</p>

<pre><code class="python">import nltk

def loadData():
    """ loading raw data """

    pos_tweets = [('I love this car', 'positive'),
                  ('This view is amazing', 'positive'),
                  ('I feel great this morning', 'positive'),
                  ('I am so excited about the concert', 'positive'),
                  ('He is my best friend', 'positive')]

    neg_tweets = [('I do not like this car', 'negative'),
                  ('This view is horrible', 'negative'),
                  ('I feel tired this morning', 'negative'),
                  ('I am not looking forward to the concert', 'negative'),
                  ('He is my enemy', 'negative')]

    return pos_tweets, neg_tweets

def generateFullData(data1, data2):
    """ generating full data with feature and label 

    fullData: [([words], label), ([words], label), ...]
    """

    fullData = []

    for (words, label) in data1 + data2:
        words_filterred = [e.lower() for e in words.split() if len(e) &gt;= 3]
        fullData.append((words_filterred, label))
    return fullData

def createKeywords(data):
    """ generating word dictionary from all words """

    wordlist = []
    for (words, label) in data:
        wordlist.extend(words)  # combining all words in a list

    wordDict = nltk.FreqDist(wordlist)  # generating frquencies of each word
    return wordDict.keys() 

def extractFeatures(dataX, keywords):
    """ extracting word table with true/false 

    features: {word: True/False}
    """

    dataX_wordSet = set(dataX)

    features = {}
    for word in keywords:
        # creating true/false table
        features['contains(%s)' % word] = (word in dataX_wordSet)  
    return features

def createTrain(data, keywords):
    """ creating train data

    train: [({features}, label), ({features}, label), ...]
    - features: {word: True/False}
    """

    train = []
    for record in data:
        # format: ({word: True/False}, label)
        train.append((extractFeatures(record[0], keywords), record[1]))  

    # if only have one argument, it also can be written by apply_features
    #train = nltk.classify.apply_features(extractFeatures, data)

    return train

def classifyNB(testX, train, keywords):
    """ classifying label with naive bayes """

    # creating Naive Bayes classifier
    classifier = nltk.NaiveBayesClassifier.train(train)

    # extracting features from raw testX and predicting label
    testX = extractFeatures(testX.split(), keywords)
    predClass = classifier.classify(testX)
    return predClass


def main():

    # loading data and combining as one
    pos_tweets, neg_tweets = loadData()
    tweets = generateFullData(pos_tweets, neg_tweets)

    # extracting keywords and train data
    keywords = createKeywords(tweets)
    train = createTrain(tweets, keywords)

    # predicting label
    testX = 'Larry is my friend'
    predClass = classifyNB(testX, train, keywords)
    print predClass


if __name__ == '__main__':
    main()
</code></pre>

<h2>Reference</h2>

<ol>
<li><a href="http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/">Twitter sentiment analysis using Python and NLTK</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Syntax]]></title>
    <link href="http://k.arttechresearch.com/tech/2014/05/23/python-syntax/"/>
    <updated>2014-05-23T13:21:29+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2014/05/23/python-syntax</id>
    <content type="html"><![CDATA[<h2>1. Default Functions</h2>

<h3>1.1. string</h3>

<pre><code class="python"># check subset
if "xxx" in aString:
    has_string = True

# join two sets
set1 = ['a', 'b', 'c']
set2 = ['b', 'c', 'd']
fullset = set(list(set1) + list(set2))
fullset = list(set(set1 + set2))

# remove empty strings
a = 'abc  '
print len(a)
print len(a.strip())
</code></pre>

<h3>1.2. IO</h3>

<pre><code class="python"># output with csv
with open(outPath+"submission-randomforest.csv", "wb") as outfile:
    outfile.write("Id,Cover_Type\n")
    for index, value in enumerate(list(clf.predict(test[cols]))):
        outfile.write("%s,%s\n" % (test['Id'].loc[index], value))
</code></pre>

<h3>1.3. random</h3>

<pre><code class="python">random.uniform(10, 20)  #   a &lt;= n &lt;= b  

random.random()  #  0 &lt;= n &lt; 1.0
random.randint(12, 20)  
random.randrange(10, 100, 2)  # [10, 12, 14, 16, ... 96, 98]
random.choice(range(10, 100, 2))
random.choice(["JGood", "is", "a", "handsome", "boy"])  

p = ["Python", "is", "powerful", "simple", "and so on..."]  
random.shuffle(p) 

list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  
slice = random.sample(list, 5)
</code></pre>

<h2>2. Default Packages</h2>

<h3>2.1. datetime</h3>

<pre><code class="python">import datetime

# calculating days: minus
d1 = datetime.datetime(2009, 3, 23)
d2 = datetime.datetime(2009, 10, 7)
(d1 - d2).days

# calculating days: plus
d1 = datetime.datetime.now()
d3 = d1 + datetime.timedelta(days=10)
d3.ctime()

# calculating seconds
starttime = datetime.datetime.now()
endtime = datetime.datetime.now()
(endtime - starttime).seconds
</code></pre>

<h3>2.2. JSON</h3>

<p>json</p>

<pre><code class="python">import json
from pprint import pprint

jsonfile = open(dataPath + fileName)
data = json.load(jsonfile)
pprint(data)
jsonfile.close()
</code></pre>

<p>simplejson</p>

<pre><code class="python">import simplejson as json

print(json.dumps(jsonData, sort_keys=True, indent=4 * ' '))
</code></pre>

<h3>2.3. os</h3>

<pre><code class="python">import os

# get the current url of the file folder
outputs = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'outputs')
</code></pre>
]]></content>
  </entry>
  
</feed>
