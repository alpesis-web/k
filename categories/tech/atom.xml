<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tech | K]]></title>
  <link href="http://k.arttechresearch.com/categories/tech/atom.xml" rel="self"/>
  <link href="http://k.arttechresearch.com/"/>
  <updated>2016-01-14T13:28:47+08:00</updated>
  <id>http://k.arttechresearch.com/</id>
  <author>
    <name><![CDATA[K]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data Visualization With D3]]></title>
    <link href="http://k.arttechresearch.com/tech/2016/01/14/data-visualization-with-d3/"/>
    <updated>2016-01-14T13:06:49+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2016/01/14/data-visualization-with-d3</id>
    <content type="html"><![CDATA[<ul>
<li>Data Visualization</li>
<li>Charting</li>
<li>D3</li>
</ul>


<h2>Data Visualization</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/19/7b/75/197b75f0249050370e5b248e6e854be2.jpg" /></p>

<h2>Charting</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/ba/b3/69/bab36904d402394edb45d3cb15756fdf.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/94/6b/25/946b252a45ff38bbadfed39bf12b6acc.jpg" /></p>

<h2>D3</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/d3/2e/19/d32e192b8b34ca65d646f774ae67ce91.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/c1/09/e7/c109e7bd15ea7b7a53ff1cdea2563d24.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/b6/38/72/b638722c1fd3b4ae0a612f9d0fca208b.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/87/75/ac/8775ac7f7c6313e014445fcf979b88c2.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/91/3a/d1/913ad1590890d42d7a351b51806c5d68.jpg" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Model Building and Validation]]></title>
    <link href="http://k.arttechresearch.com/tech/2016/01/14/model-building-and-validation/"/>
    <updated>2016-01-14T12:43:15+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2016/01/14/model-building-and-validation</id>
    <content type="html"><![CDATA[<p>QMV Process</p>

<ul>
<li>Q: Questioning</li>
<li>M: Modelling</li>
<li>V: Validation</li>
</ul>


<h2>QMV Process</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/6a/1f/04/6a1f048309142c91cd22ea6c2cc4058a.jpg" /></p>

<h2>Questioning</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/cf/90/db/cf90dbb0d7938dc756a50ca5626def63.jpg" /></p>

<h2>Modelling</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/43/ac/78/43ac78c79a3c72b654a6aa29b2ae1543.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/d3/5c/92/d35c92eeb5d2abcb049b2cc163214f42.jpg" /></p>

<h2>Validation</h2>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/62/fe/6e/62fe6ecdddbde137e5e52204ca522947.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/da/f6/be/daf6be7b4e7a69ce774b04f7e8a08738.jpg" /></p>

<p><img src="https://s-media-cache-ak0.pinimg.com/736x/6c/c0/86/6cc0860c7a360a43e32edea481c7ac80.jpg" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git Commands]]></title>
    <link href="http://k.arttechresearch.com/tech/2016/01/08/git-commands/"/>
    <updated>2016-01-08T18:06:55+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2016/01/08/git-commands</id>
    <content type="html"><![CDATA[<ul>
<li>config</li>
<li>remote</li>
<li>pull/commit</li>
<li>log</li>
</ul>


<h2>Config</h2>

<pre><code class="bash">$ git config --global user.name 'xxx'
$ git config --global user.email 'xxx'
$ git config --local user.name 'xxx'
$ git config --local user.email 'xxx'

$ git config --global alias.logtree "log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit"
</code></pre>

<h2>Remote</h2>

<pre><code class="bash">$ git remote add origin https://xxxx                    # set remote url
$ git remote set-url origin https://xxxx                # update remote url
</code></pre>

<h2>Pull</h2>

<pre><code class="bash">$ git pull --rebase origin branch_name
$ git fetch origin
$ git rebase origin
</code></pre>

<h2>Commit</h2>

<pre><code class="bash">$ git diff &lt;file&gt;
$ git add &lt;file(s)&gt;
$ git status
$ git reset HEAD &lt;file&gt;
$ git commit -m "xxx"
$ git push origin &lt;branch_name&gt;
</code></pre>

<h2>Log</h2>

<pre><code class="bash">$ git log
$ git show &lt;id&gt;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Real-time Analytics With Apache Storm]]></title>
    <link href="http://k.arttechresearch.com/tech/2016/01/07/real-time-analytics-with-apache-storm/"/>
    <updated>2016-01-07T23:26:08+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2016/01/07/real-time-analytics-with-apache-storm</id>
    <content type="html"><![CDATA[<h2>Toolkits</h2>

<table>
<thead>
<tr>
<th style="text-align:left;"> DevOps        </th>
<th style="text-align:left;">        </th>
<th style="text-align:left;">       </th>
<th style="text-align:left;"> Backend                                </th>
<th style="text-align:left;"> Frontend    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> Vagrant       </td>
<td style="text-align:left;"> Ubuntu </td>
<td style="text-align:left;"> Storm </td>
<td style="text-align:left;"> Python (BeautifulSoup, Flask, Lettuce) </td>
<td style="text-align:left;"> Javascript  </td>
</tr>
<tr>
<td style="text-align:left;"> Vagrant Cloud </td>
<td style="text-align:left;"> Git    </td>
<td style="text-align:left;">       </td>
<td style="text-align:left;"> Java                                   </td>
<td style="text-align:left;"> D3          </td>
</tr>
<tr>
<td style="text-align:left;"> VirtualBox    </td>
<td style="text-align:left;"> Maven  </td>
<td style="text-align:left;">       </td>
<td style="text-align:left;"> Redis                                  </td>
<td style="text-align:left;">             </td>
</tr>
</tbody>
</table>


<p>Others</p>

<ul>
<li>Clojure</li>
<li>Cluster Administration</li>
<li>Ack</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudera Hadoop by Example]]></title>
    <link href="http://k.arttechresearch.com/tech/2016/01/06/cloudera-hadoop-by-example/"/>
    <updated>2016-01-06T22:05:08+08:00</updated>
    <id>http://k.arttechresearch.com/tech/2016/01/06/cloudera-hadoop-by-example</id>
    <content type="html"><![CDATA[<p>Contents</p>

<ul>
<li>analyzing structured data (sqoop, impala)</li>
<li>analyzing unstructured data (beeline, impala)</li>
<li>analyzing data in realtime (spark)</li>
<li>indexing and searching data (solr)</li>
</ul>


<h2>Analyzing Structured Data: MySQL Data</h2>

<p>Tools:</p>

<ul>
<li>sqoop</li>
<li>hive</li>
<li>hdfs</li>
<li>avro</li>
<li>mysql</li>
<li>mapreduce</li>
</ul>


<h3>Loading data by sqoop</h3>

<p>Steps:</p>

<ul>
<li>connecting MySQL database</li>
<li>lauching MapReduce jobs</li>
<li>putting the export files in Avro format in HDFS, and creating the Avro schema</li>
<li>(later) loading Hive tables for use in Impala</li>
</ul>


<pre><code class="bash loading data from local to HDFS by sqoop">$ cd /path/to/project

$ sqoop import-all-tables \
-m 1 \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username=retail_dba \
--password=cloudera \
--compression-codec=snappy \
--as-avrodatafile \
--warehouse-dir=/user/hive/warehouse
</code></pre>

<h3>Validation</h3>

<p>NOTE: <code>*.avsc</code> files is located on the local system.</p>

<pre><code class="bash validating data in HDFS">$ hadoop fs -ls /user/hive/warehouse                     # HDFS
$ hadoop fs -ls /user/hive/warehouse/categories/         # HDFS

$ ls -l *.avsc                                           # local, schema files
</code></pre>

<h3>Copying the schema files (<code>*.avsc</code>) to HDFS</h3>

<pre><code class="bash copying avro schemas to HDFS">$ sudo -u hdfs hadoop fs -mkdir /user/examples
$ sudo -u hdfs hadoop fs -chmod +rw /user/examples
$ hadoop fs -copyFromLocal ./*.avsc /user/examples/
</code></pre>

<p>NOTES: Hive and Impala</p>

<p>Hive and Impala both read the data from files in HDFS, and they even share metadata about the tables.</p>

<p>Hive - executes queries by compiling them to MapReduce jobs, this means it can be more flexible, but is much slower.</p>

<p>Impala - is an MPP query engine that reads the data directly from the file system itself. This allows it to execute queries fast enough for interactive analysis and exploration.</p>

<h3>Creating tables</h3>

<p>tools:</p>

<ul>
<li>hue</li>
<li>impala</li>
</ul>


<p>Hue -> Impala: creating tables</p>

<pre><code class="sql creating table in Impala">CREATE EXTERNAL TABLE categories STORED AS AVRO
LOCATION 'hdfs:///user/hive/warehouse/categories'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart/user/examples/sqoop_import_categories.avsc');

CREATE EXTERNAL TABLE customers STORED AS AVRO
LOCATION 'hdfs:///user/hive/warehouse/customers'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart/user/examples/sqoop_import_customers.avsc');

CREATE EXTERNAL TABLE departments STORED AS AVRO
LOCATION 'hdfs:///user/hive/warehouse/departments'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart/user/examples/sqoop_import_departments.avsc');

CREATE EXTERNAL TABLE orders STORED AS AVRO
LOCATION 'hdfs:///user/hive/warehouse/orders'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart/user/examples/sqoop_import_orders.avsc');

CREATE EXTERNAL TABLE order_items STORED AS AVRO
LOCATION 'hdfs:///user/hive/warehouse/order_items'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart/user/examples/sqoop_import_order_items.avsc');

CREATE EXTERNAL TABLE products STORED AS AVRO
LOCATION 'hdfs:///user/hive/warehouse/products'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart/user/examples/sqoop_import_products.avsc');

show tables;
</code></pre>

<h3>Exploring data</h3>

<p>queries</p>

<pre><code class="sql exploring data in Impala">-- Most popular product categories
select c.category_name, count(order_item_quantity) as count
from order_items oi
inner join products p on oi.order_item_product_id = p.product_id
inner join categories c on c.category_id = p.product_category_id
group by c.category_name
order by count desc
limit 10;


-- top 10 revenue generating products
select p.product_id, p.product_name, r.revenue
from products p 
inner join (
    select oi.order_item_product_id, sum(cast(oi.order_item_subtotal as float)) as revenue
    from order_items oi
    inner join orders o on oi.order_item_order_id = o.order_id
    where o.order_status &lt;&gt; ‘CANCELED’ and o.order_status &lt;&gt; ’SUSPECTED_FRAUD’
    group by order_item_product_id
) r on p.product_id = r.order_item_product_id
order by r.revenue desc
limit 10;
</code></pre>

<h2>Analyzing Unstructured data: log files</h2>

<p>tools:</p>

<ul>
<li>flume/beeline</li>
<li>hive</li>
<li>impala</li>
</ul>


<h3>Copying the log files to HDFS</h3>

<pre><code class="bash copy the log files to HDFS">$ sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse/original_access_logs
$ sudo -u hdfs hadoop fs -copyFromLocal /opt/examples/log_files/access.log.2 /user/hive/warehouse/original_access_logs
$ hadoop fs -ls /user/hive/warehouse/original_access_logs
</code></pre>

<h3>Creating Intermediate Table</h3>

<p>local logs -> intermediate table -> final table</p>

<pre><code class="bash loading log files to Hive">$ beeline -u adbc:hive2://quickstart:10000/default -n admin -d org.apache.hive.jdbc.HiveDriver

0: jdbc:hive2://quickstart:10000/default&gt; CREATE EXTERNAL TABLE intermediate_access_logs (
ip STRING,
date STRING,
method STRING,
url STRING,
http_version STRING,
code1 STRING,
code2 STRING,
dash STRING,
user_agent STRING)
ROW FORMAT SERDE ‘org.apache.hadoop.hive.contrib.serde2.RegexSerDe’
WITH SERDEPROPERTIES (
‘input.regex’ = ‘([^ }*) - - \\[([^\\]]*)\\] “([^\ ]*) ([^\ ]*) ([^\ ]*)” (\\d*) (\\d*) “([^”]*)” “([^”]*”’,
‘output.format.string’ = “%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s”
)
LOCATION ‘/user/hive/warehouse/original_access_logs’;

0: jdbc:hive2://quickstart:10000/default&gt; CREATE EXTERNAL TABLE tokenized_access_log (
ip STRING,
date STRING,
method STRING,
url STRING,
http_version STRING,
code1 STRING,
code2 STRING,
dash STRING,
user_agent STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’
LOCATION ‘/user/hive/warehouse/tokenized_access_logs';

0: jdbc:hive2://quickstart:10000/default&gt; ADD JAR /usr/lib/hive/lib/hive-contrib.jar
0: jdbc:hive2://quickstart:10000/default&gt; INSERT OVERWRITE TABLE tokenized_access_logs SELECT * FROM intermediate_access_logs;
0: jdbc:hive2://quickstart:10000/default&gt; !quit
</code></pre>

<h3>Validating in Impala</h3>

<pre><code class="sql validating tables in Impala">invalidate metadata;
show tables;

select count(*),  url from tokenized_access_logs
where url like ‘%\/product\/%’
group by url
order by count(*) desc;
</code></pre>

<h2>Analyzing data with Spark</h2>

<h3>Start Spark</h3>

<pre><code class="bash start Spark">$ spark-shell —jars /usr/lib/avro/avro-mapred.jar \
—conf spark.serializer=org.apache.spark.serializer.KryoSerializer
</code></pre>

<h3>Programming in Scala</h3>

<pre><code class="scala relationship strengh analytics">// First we're going to import the classes we need and open some of the files
// we imported from our relational database into Hadoop with Sqoop

import org.apache.avro.generic.GenericRecord
import org.apache.avro.mapred.{AvroInputFormat, AvroWrapper}
import org.apache.hadoop.io.NullWritable

val warehouse = "hdfs://quickstart/user/hive/warehouse"

val order_items_path = warehouse + "order_items"
val order_items = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](order_items_path)

val products_path = warehouse + "products"
val products = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](product_path)



// Next, we extract the fields from order_items and products that we care about
// and get a list of every product, its name and quantity, grouped by order

val orders = order_items.map { x =&gt; (
  x._1.datum.get("order_item_product_id"),
  (x._1.datum.get("order_item_order_id"), x._1.datum.get("order_item_quantity"))
)}.join(
  products.map { x =&gt; (
    x._1.datum.get("product_id"),
    (x._1.datum.get("product_name"))
)}).map(x =&gt; (
  scala.Int.unbox(x._2._1._1), // order_id
  (
    scala.Int.unbox(x._2._1._2), // quantity
    x._2._2.toString // product_name
  )
)).groupByKey()


// Finally, we tally how many times each combination of products appears
// together in an order, and print the 10 most common combinations.

val cooccurrences = orders.map(order =&gt; (
  order._1,
  order._2.toList.combinations(2).map(order_pair =&gt; (
    if (order_pair(0)._2 &lt; order_pair(1)._2) (order_pair(0)._2, order_pair(1)._2)
    else (order_pair(1)._2, order_pair(0)._2), order_pair(0)._1 * order_pair(1)._1
  ))
))

val combos = cooccurrrences.flatMap(x =&gt; x._2).reduceByKey((a,b) =&gt; a+b)
val mostCommon = combos.map(x =&gt; (x._2, x._1)).sortByKey(false).take(10)

println(mostCommon.deep.mkString("\n"))

exit
</code></pre>

<p>NOTE:</p>

<p>When we do a &lsquo;map&rsquo;, we specify a function that will take each record and output a modified record. This is useful when we only need a couple of fields from each record  or when we need the record to use a different field as the key: we simply invoke map with a function that takes in the entire record, and returns a new record with the fields and the key we want.</p>

<p>The &lsquo;reduce&rsquo; operations - like &lsquo;join&rsquo; and &lsquo;groupBy&rsquo; - will organise these records by their keys so we can group similar records together and then process them as a group.</p>

<h2>Indexing data and search by Solr</h2>

<h3>Creating a search schema</h3>

<p>steps</p>

<ul>
<li>creating an empty configuration</li>
<li>editing your schema</li>
<li>uploading your configuration</li>
<li>creating your collection</li>
</ul>


<pre><code class="bash creating a search schame by Solr">$ solrctl --zk quickstart:2181/solr instancedir --generate solr_configs
$ cd /opt/examples/flume
$ solrctl --zk quickstart:2181/solr instancedir --create live_logs ./solr_configs
$ solrctl --zk quickstart:2181/solr collection --create live_logs -s 1
</code></pre>

<h3>Loading data into Solr</h3>

<p>tools</p>

<ul>
<li>flume - a tool for ingesting streams of data into your cluster from sources such as log files, network streams, and more; is a system for collecting, aggregating, and moving large amounts of log data from many different sources to a centralised data source.</li>
<li>morphines - a Java library for doing ETL on-the-fly.</li>
</ul>


<pre><code class="bash loading data by flume">$ start_logs
$ tail_logs
$ stop_logs

$ flume-ng agent \
--conf /opt/examples/flume/conf \
--conf-file /opt/examples/flume/conf/flume.conf \
--name agent1 \
-Dflume.root.logger=DEBUG,INFO,console
</code></pre>

<h3>Playing data in Solr</h3>

<p>Hue -> Search -> Solr Search -> Dashboard</p>

<ul>
<li>browsing the data</li>
<li>charting</li>
</ul>

]]></content>
  </entry>
  
</feed>
